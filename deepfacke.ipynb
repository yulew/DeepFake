{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !apt-get install zip\n# !zip -r train_data /kaggle/input/deepfake-detection-challenge/train_sample_videos/*\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport cv2\nplt.style.use('ggplot')\nfrom IPython.display import Video\nfrom IPython.display import HTML\nfrom PIL import Image, ImageDraw\n# !pip install face_recognition\n# import face_recognition\nimport os\nimport pdb\nimport pickle\nfrom multiprocessing import Pool\n!pip install facenet-pytorch\nimport torch\nfrom tqdm.notebook import tqdm\nimport time\nfrom facenet_pytorch import MTCNN\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\npad_ratio = 1.3\ntrain_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Utilities***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_faces(images, figsize=(10.8/2, 19.2/2)):\n    shape = images[0].shape\n    images = images[np.linspace(0, len(images)-1, 16).astype(int)]\n    im_plot = []\n    for i in range(0, 16, 4):\n        im_plot.append(np.concatenate(images[i:i+4], axis=0))\n    im_plot = np.concatenate(im_plot, axis=1)\n    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.imshow(im_plot)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n\n    ax.grid(False)\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Get Frames**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_frame(file_name, image):\n    np.save(file_name, image)\n    \ndef get_frames(video_path, max_frame_index=9999999):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    i = 0\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret==True:\n            frames.append(frame)\n            i += 1\n            if i == max_frame_index:\n                break\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        else:\n            break\n    cap.release()\n    \n    return frames\n    \ndef save_video(video_file, path_dir):\n    frames = get_frames(path_dir + video_file)\n    \n    for i, frame in enumerate(frames):\n        save_frame(video_file + str(i), frame)\n        \ndef get_frames_i(index, max_frame_index=9999999):\n    return get_frames(train_dir + train_video_names[index], max_frame_index=max_frame_index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Metadata**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample_metadata = pd.read_json('/kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\ntrain_video_names = [x for x in os.listdir(train_dir)]\ntrain_sample_names = []\nfor x in train_video_names:\n    if x != 'metadata.json':\n        train_sample_names.append(x)\ntrain_video_names = train_sample_names\n\ntrain_sample_metadata = train_sample_metadata.loc[train_video_names]\ntrain_labels = np.array([0 if x == \"REAL\" else 1 for x in train_sample_metadata['label']])\n\n\ntest_dir = '/kaggle/input/deepfake-detection-challenge/test_videos/'\ntest_video_names = [x for x in os.listdir(test_dir)]\n\ntrain_sample_metadata = pickle.load(open('/kaggle/input/deepfake-metadata/train_sample_metadata', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Data Preprocess***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from facenet_pytorch import MTCNN\ndetector = MTCNN(device=device, post_process=False)\n\nbatch_size = 32\n\ndef detect_facenet_pytorch(detector, images, batch_size):\n    boxes = []\n    landmarks = []\n    i_frames = []\n    for lb in np.arange(0, len(images), batch_size):\n        imgs = [img for img in images[lb:lb+batch_size]]\n        box, _, lms = detector.detect(imgs, landmarks=True)\n#         print(type(box))\n        for i in range(len(box)):\n            if box[i] is None:\n                continue\n            boxes.append(box[i][0])\n            landmarks.append(lms[i])\n            i_frames.append(i + lb)\n    return boxes, landmarks, i_frames\n\n\n# frames = get_frames_i(0)\n# frames = np.stack(frames)\n# a, b, c = detect_facenet_pytorch(detector, frames, batch_size)\n# i_frames = []\n# boxes = []\n# landmarks = []\n# load_video_batch_size = 1\n\nfor lb in tqdm(np.arange(0, len(train_video_names), load_video_batch_size)):\n    print([train_dir + x for x in train_video_names[lb:lb+load_video_batch_size]])\n    with Pool(load_video_batch_size) as p:\n        frames_in_videos = p.map(get_frames, [train_dir + x for x in train_video_names[lb:lb+load_video_batch_size]])\n#     pdb.set_trace()\n    for frames in frames_in_videos:\n      \n# for name in tqdm(train_video_names):\n        frames = get_frames(train_dir + name)\n        frames = np.stack(frames)\n\n        a, b, c = detect_facenet_pytorch(detector, frames, batch_size)\n        boxes.append(a)\n        landmarks.append(b)\n        i_frames.append(c)\n\n\n# train_sample_metadata['index_w/_frames'] = i_frames\n# train_sample_metadata['face_locations'] = boxes\n# train_sample_metadata['face_ladmarks'] = landmarks\n\n# pickle.dump(train_sample_metadata, open( \"train_sample_metadata\", \"wb\" ) )\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.imshow(frames[0])\n# plt.show()\n\n# l, t, r, b = a[0].astype(int).tolist()\n# plt.imshow(frames[0][t:b, l:r])\n# plt.show()\n\n# print(train_sample_metadata.loc[train_video_names]['face_locations'])\n# print(train_video_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MesoNet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model as KerasModel\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import LSTM\nIMGWIDTH = 256\n\nclass Classifier:\n    def __init__():\n        self.model = 0\n    \n    def predict(self, x):\n        return self.model.predict(x)\n    \n    def fit(self, x, y):\n        return self.model.train_on_batch(x, y)\n    \n    def get_accuracy(self, x, y):\n        return self.model.test_on_batch(x, y)\n    \n    def get_summary(self):\n        return self.model.summary()\n    \n    def load(self, path):\n        self.model.load_weights(path)\n        \n    def features_model(self):\n        return tf.keras.Model(self.model.input, self.model.layers[-2].output)\n\nclass CNN(Model):\n    def __init__(self, feature_model, n_filters=8, kernel_size=5):\n        super(RNN, self).__init__()\n        self.feature_model = feature_model\n        self.feature_model.train_able = False\n        self.conv1 = Conv1D(n_filters, kernel_size, activation='relu')\n        self.last = Dense(1)    \n        \n    def call(inputs):\n        n_videos, n_ims, s1, s2, s3 = inputs.shape\n        features = self.feature_model(inputs.reshape((-1, s1, s2, s3))).reshape(n_videos, n_ims, -1)\n        x = self.conv1(features).reshape(n_videos, -1)\n        return self.last(x)\n            \nclass RNN(Model):\n    def __init__(self, feature_model, LSTM_len=16):\n        super(Temporal_CNN, self).__init__()\n        self.feature_model = feature_model\n        self.feature_model.train_able = False\n        self.LSTM = LSTM(LSTM_len)\n        self.last = Dense(1)\n    \n    \"\"\"\n    inputs: nparray with size n_videos * n_faces * im_sizes\n    \"\"\"\n    def call(inputs):\n        n_videos = len(inputs)\n        xs = []\n        for i in range(n_videos):\n            features = self.feature_model(intputs[i])\n            xs.append(self.LSTM(features))\n        xs = np.stack(xs)\n        \n        return self.last(xs)            \n    \nclass Xception_binary(Classifier):\n    def __init__(self, learning_rate = 0.001):\n        self.model = self.init_model()\n        optimizer = Adam(lr = learning_rate)\n        self.model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    def init_model(self):\n        from tensorflow.keras.applications import Xception\n        xception = Xception()\n        xception.trainable = False\n        xception_features = tf.keras.Model(xception.input, xception.layers[-2].output)\n        # model2.summary()\n        prediction_layer = tf.keras.layers.Dense(1)\n        model = tf.keras.Sequential([\n          xception_features,\n          prediction_layer])\n        return model\n\n    def fine_tunning_on(self, layers_lb=0, layers_ub=100):\n        self.model.trainable = True\n        for layer in self.model.layers[:layers_lb]:\n            layer.trainable =  False\n        for layer in self.model.layers[layers_ub:]:\n            layer.trainable =  False\n    \n\nclass MesoInception4(Classifier):\n    def __init__(self, learning_rate = 0.001):\n        self.model = self.init_model()\n        optimizer = Adam(lr = learning_rate)\n        self.model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    def InceptionLayer(self, a, b, c, d):\n        def func(x):\n            x1 = Conv2D(a, (1, 1), padding='same', activation='relu')(x)\n            \n            x2 = Conv2D(b, (1, 1), padding='same', activation='relu')(x)\n            x2 = Conv2D(b, (3, 3), padding='same', activation='relu')(x2)\n            \n            x3 = Conv2D(c, (1, 1), padding='same', activation='relu')(x)\n            x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides = 1, padding='same', activation='relu')(x3)\n            \n            x4 = Conv2D(d, (1, 1), padding='same', activation='relu')(x)\n            x4 = Conv2D(d, (3, 3), dilation_rate = 3, strides = 1, padding='same', activation='relu')(x4)\n\n            y = Concatenate(axis = -1)([x1, x2, x3, x4])\n            \n            return y\n    \n    def init_model(self):\n        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n        \n        x1 = self.InceptionLayer(1, 4, 4, 2)(x)\n        x1 = BatchNormalization()(x1)\n        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n        \n        x2 = self.InceptionLayer(2, 4, 4, 2)(x1)\n        x2 = BatchNormalization()(x2)\n        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n        \n        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n        x3 = BatchNormalization()(x3)\n        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n        \n        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n        x4 = BatchNormalization()(x4)\n        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n        \n        y = Flatten()(x4)\n        y = Dropout(0.5)(y)\n        y = Dense(16)(y)\n        y = LeakyReLU(alpha=0.1)(y)\n        y = Dropout(0.5)(y)\n        y = Dense(1, activation = 'sigmoid')(y)\n\n        return KerasModel(inputs = x, outputs = y)\n        \nclass Meso4(Classifier):\n    def __init__(self, learning_rate = 0.001):\n        self.model = self.init_model()\n        optimizer = Adam(lr = learning_rate)\n        self.model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    def init_model(self): \n        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n        \n        x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n        x1 = BatchNormalization()(x1)\n        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n        \n        x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n        x2 = BatchNormalization()(x2)\n        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n        \n        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n        x3 = BatchNormalization()(x3)\n        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n        \n        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n        x4 = BatchNormalization()(x4)\n        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n        \n        y = Flatten()(x4)\n        y = Dropout(0.5)(y)\n        y = Dense(16)(y)\n        y = LeakyReLU(alpha=0.1)(y)\n        y = Dropout(0.5)(y)\n        y = Dense(1, activation = 'sigmoid')(y)\n\n        return KerasModel(inputs = x, outputs = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Hyperparameters***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_int(x, lb, ub):\n    return int(max(lb, min(x, ub)))\n\ndef clip_face_location(face_location, ub1=1080-1, ub2=1920-1, lb1=0, lb2=0, pad_ratio=1.3):\n    \n    left, top, right, bottom = face_location\n    l1 = bottom - top\n    l1 *= pad_ratio / 2\n    l2 = right - left\n    l2 *= pad_ratio / 2\n    mid1 = (top + bottom) * .5\n    mid2 = (left + right) * .5\n    top = clip_int(mid1 - l1, lb1, ub1)\n    bottom = clip_int(mid1 + l1, lb1, ub1)\n    right = clip_int(mid2 + l2, lb2, ub2)\n    left = clip_int(mid2 - l2, lb2, ub2)\n    \n    return left, top, right, bottom \n\ndef get_face_given_location(frame, face_location):\n    \n    face_location = clip_face_location(face_location)\n    left, top, right, bottom  = face_location\n    face = frame[top:bottom, left:right]\n        \n    return face\n\ndef add_landmarks(frame, LM_width=3):\n    face_landmarks_list = face_recognition.face_landmarks(frame)\n    pil_image = Image.fromarray(image)\n    d = ImageDraw.Draw(pil_image)\n    \n\n    for face_landmarks in face_landmarks_list:\n        for facial_feature in face_landmarks.keys():\n            d.line(face_landmarks[facial_feature], width=LM_width)\n            \n    return np.array(pil_image)\n\ndef face_resize(face, face_len1=256, face_len2=256, n_channels=3):\n    \n    img = Image.fromarray(face)\n#         pdb.set_trace()\n    img = img.resize((face_len1, face_len2))\n    face_resized = np.array(img)\n        \n    return face_resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nn_epochs = 100\nn_videos = len(train_video_names)\nvideo_len = 300\nbatch_size_frames = 8\nbatch_size_videos = 8\nbatch_size = batch_size_frames * batch_size_videos\ndata_type = 'face'\nn_batches = int(n_videos * video_len / batch_size)\nface_len1, face_len2, n_channels = 256, 256, 3    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Get Batches***"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nrandom.seed(0)\n\ndef get_batch_1video(x = ('face', 256)):\n    data_type, face_size = x\n    while True:\n        i_video = random.randint(0, n_videos - 1)\n        if len(train_sample_metadata.loc[train_video_names]['face_locations'][i_video]) != 0:\n            break\n    \n    print(i_video, train_labels[i_video])\n    i_frames = train_sample_metadata.loc[train_video_names]['index_w/_frames'][i_video]\n    face_locations = train_sample_metadata.loc[train_video_names]['face_locations'][i_video]\n    \n    faces = [] \n    sampled_frames = random.choices(range(len(i_frames)), k=batch_size_frames)\n    \n    frames = get_frames_i(i_video, max_frame_index=max(i_frames)+1)\n    \n    for i in sampled_frames:\n        fcs = get_face_given_location(frames[i_frames[i]], face_locations[i].tolist())\n#         plt.imshow(fcs)\n#         plt.show()\n        fcs = face_resize(fcs, face_len1=face_size, face_len2=face_size, n_channels=n_channels)\n        faces.append(fcs)\n#     pdb.set_trace()        \n    return faces, np.ones(batch_size_frames) * train_labels[i_video]\n    \ndef get_batch(data_type='face', face_size=256):\n    face_batch = np.zeros((0, face_size, face_size, n_channels))\n    label_batch = np.zeros(0)\n    \n#     batch = []\n#     for i in range(batch_size_videos):\n#         batch.append(get_batch_1video(data_type=data_type))\n    \n#     pdb.set_trace()\n    with Pool(2) as p:\n        batch = p.map(get_batch_1video, [('face', face_size) for i in range(batch_size_videos)])\n            \n    for x in batch:\n        faces, labels = x\n        face_batch = np.concatenate((face_batch, faces))\n        label_batch = np.concatenate((label_batch, labels))\n        \n    return face_batch, label_batch\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/input/meso-pretrain\ncnn_im = Xception_binary(learning_rate=1e-3)\ncnn_im.get_summary()\n# model.load(\"MesoInception_F2F\")\nfor epoch in range(n_epochs):\n    for _ in range(n_batches):\n        x, y = get_batch(face_size=299)\n        print(cnn_im.fit(x, y))\n#         print(model.predict(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RNN(cnn_im.feature_model())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_norms = []\n\nfor name in train_video_names:\n    frames = get_frames(train_dir + name)\n    frames = np.stack(frames)\n    dns = []\n    for i in range(len(frames) - 1):\n        dns.append(np.norm((frames[i]-frames[i+1]).flatten()))\n    dns = np.stack(dns)\n    diff_norms.append(dns)\n    \ndiff_norms = np.stack(diff_norms)\n\nplt.hist(diff_norms[train_labels == 0])\nplt.show()\nplt.hist(diff_norms[train_labels == 0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}